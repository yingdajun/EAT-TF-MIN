{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'the', b'and', b'a', b'of', b'to', b'is', b'in', b'it', b'i', b'this', b'that', b'was', b'as', b'for', b'with', b'movie', b'but', b'film', b'on', b'not', b'you', b'his', b'are', b'have', b'be', b'he', b'one', b'its', b'at', b'all', b'by', b'an', b'they', b'from', b'who', b'so', b'like', b'her', b'just', b'or', b'about', b'has', b'if', b'out', b'some', b'there', b'what', b'good', b'more', b'when', b'very', b'she', b'even', b'my', b'no', b'would', b'up', b'time', b'only', b'which', b'story', b'really', b'their', b'were', b'had', b'see', b'can', b'me', b'than', b'we', b'much', b'well', b'get', b'been', b'will', b'into', b'people', b'also', b'other', b'do', b'bad', b'because', b'great', b'first', b'how', b'him', b'most', b'dont', b'made', b'then', b'them', b'films', b'movies', b'way', b'make', b'could', b'too', b'any', b'after', b'characters']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,preprocessing,optimizers,losses,metrics\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import re,string\n",
    "\n",
    "train_data_path = \"./data/imdb/train.csv\"\n",
    "test_data_path =  \"./data/imdb/test.csv\"\n",
    "\n",
    "MAX_WORDS = 10000  # 仅考虑最高频的10000个词\n",
    "MAX_LEN = 200  # 每个样本保留200个词的长度\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "\n",
    "#构建管道\n",
    "def split_line(line):\n",
    "    arr = tf.strings.split(line,\"\\t\")\n",
    "    label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[0]),tf.int32),axis = 0)\n",
    "    text = tf.expand_dims(arr[1],axis = 0)\n",
    "    return (text,label)\n",
    "\n",
    "ds_train_raw =  tf.data.TextLineDataset(filenames = [train_data_path]) \\\n",
    "   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test_raw = tf.data.TextLineDataset(filenames = [test_data_path]) \\\n",
    "   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "#构建词典\n",
    "def clean_text(text):\n",
    "    lowercase = tf.strings.lower(text)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    cleaned_punctuation = tf.strings.regex_replace(stripped_html,\n",
    "         '[%s]' % re.escape(string.punctuation),'')\n",
    "    return cleaned_punctuation\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=clean_text,\n",
    "    split = 'whitespace',\n",
    "    max_tokens=MAX_WORDS-1, #有一个留给占位符\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN)\n",
    "\n",
    "ds_text = ds_train_raw.map(lambda text,label: text)\n",
    "vectorize_layer.adapt(ds_text)\n",
    "print(vectorize_layer.get_vocabulary()[0:100])\n",
    "\n",
    "\n",
    "#单词编码\n",
    "ds_train = ds_train_raw.map(lambda text,label\n",
    "                            :(vectorize_layer(text),label)) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "ds_test = ds_test_raw.map(lambda text,label\n",
    "                          :(vectorize_layer(text),label)) \\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=models.Sequential()\n",
    "model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
    "model.add(layers.Conv1D(16,kernel_size=5,activation=\"relu\"))\n",
    "model.add(layers.MaxPool1D())\n",
    "model.add(layers.Conv1D(128,kernel_size=2,activation=\"relu\"))\n",
    "model.add(layers.MaxPool1D())\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 7)            70000     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 196, 16)           576       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 98, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 97, 128)           4224      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 48, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 6145      \n",
      "=================================================================\n",
      "Total params: 80,945\n",
      "Trainable params: 80,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Nadam()\n",
    "loss_func = losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入深度学习库\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers,losses,metrics,callbacks\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 由于这是一个二分类问题，所以取出来对应的损失函数与衡量标准\n",
    "\n",
    "from tensorflow.keras.metrics import Accuracy,Precision,Recall,AUC\n",
    "\n",
    "# 评估标准实体化\n",
    "\n",
    "acc=Accuracy()\n",
    "auc=AUC()\n",
    "precision=Precision()\n",
    "recall=Recall()\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam,Adagrad,Adadelta,Nadam,RMSprop,SGD\n",
    "\n",
    "# 优化器实体化\n",
    "adam=Adam()\n",
    "adagrad=Adagrad()\n",
    "adadelta=Adadelta()\n",
    "nadam=Nadam()\n",
    "rms=RMSprop()\n",
    "sgd=SGD()\n",
    "\n",
    "\n",
    "from tensorflow.keras.losses import BinaryCrossentropy,Hinge\n",
    "\n",
    "# 损失函数实体化\n",
    "\n",
    "bc=BinaryCrossentropy()\n",
    "hinge=Hinge()\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "\n",
    "# 回调函数函数实体化\n",
    "\n",
    "#如果loss在100个epoch后没有提升，学习率减半。\n",
    "lr_callback = ReduceLROnPlateau(monitor=\"accuracy\",factor = 0.5, patience = 100)\n",
    "#当loss在200个epoch后没有提升，则提前终止训练。\n",
    "stop_callback = EarlyStopping(monitor = \"accuracy\", patience= 200)\n",
    "callbacks_list = [lr_callback,stop_callback]\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input,Dense,Conv2D,MaxPool2D,Dropout,Flatten\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 0.0823 - auc: 0.9955 - accuracy: 0.9719 - recall: 0.9720 - val_loss: 0.6062 - val_auc: 0.9193 - val_accuracy: 0.8550 - val_recall: 0.8905 - lr: 0.0010\n",
      "Epoch 2/4\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 0.0467 - auc: 0.9983 - accuracy: 0.9858 - recall: 0.9868 - val_loss: 0.7662 - val_auc: 0.9097 - val_accuracy: 0.8518 - val_recall: 0.8351 - lr: 0.0010\n",
      "Epoch 3/4\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 0.0254 - auc: 0.9994 - accuracy: 0.9926 - recall: 0.9936 - val_loss: 0.9584 - val_auc: 0.9028 - val_accuracy: 0.8482 - val_recall: 0.8491 - lr: 0.0010\n",
      "Epoch 4/4\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.0186 - auc: 0.9997 - accuracy: 0.9943 - recall: 0.9939 - val_loss: 1.0732 - val_auc: 0.8987 - val_accuracy: 0.8516 - val_recall: 0.8724 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17c52518828>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=loss_func,\n",
    "             optimizer=optimizer,\n",
    "             metrics=['AUC','accuracy','Recall'])\n",
    "model.fit(ds_train,validation_data=ds_test,epochs=4\n",
    "          ,batch_size=64\n",
    "         ,callbacks=[lr_callback,stop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
